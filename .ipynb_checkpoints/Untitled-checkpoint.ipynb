{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e9182c7-6678-4e53-aaef-56bd994c365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "# model = YOLO('yolov8n.pt')\n",
    " \n",
    "# # Training.\n",
    "# results = model.train(\n",
    "#    data='C:/Users/PlusninMA/Desktop/YOLOv8/Accum.v1i.yolov8/data.yaml',\n",
    "#    imgsz=640,\n",
    "#    epochs=10,\n",
    "#    batch=2,\n",
    "#    project=\"C:/Users/PlusninMA/Desktop/YOLOv8\",\n",
    "#    name='yolov8n_custom',    \n",
    "#    save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02305620-9d72-4d86-9907-fc083a6bced3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.2.77  Python-3.12.4 torch-2.4.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=segment, mode=train, model=yolov8n-seg.pt, data=C:/Users/PlusninMA/Desktop/YOLOv8/Accum.v1i.yolov8/data.yaml, epochs=10, time=None, patience=100, batch=2, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=C:/Users/PlusninMA/Desktop/YOLOv8, name=yolov8n-seg_custom, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=C:\\Users\\PlusninMA\\Desktop\\YOLOv8\\yolov8n-seg_custom\n",
      "Overriding model.yaml nc=80 with nc=2\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1   1004470  ultralytics.nn.modules.head.Segment          [2, 32, 64, [64, 128, 256]]   \n",
      "YOLOv8n-seg summary: 261 layers, 3,264,006 parameters, 3,263,990 gradients, 12.1 GFLOPs\n",
      "\n",
      "Transferred 381/417 items from pretrained weights\n",
      "WARNING  ClearML installed but not initialized correctly, not logging this run. It seems ClearML is not configured on this machine!\n",
      "To get started with ClearML, setup your own 'clearml-server' or create a free account at https://app.clear.ml\n",
      "Setup instructions can be found here: https://clear.ml/docs\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PlusninMA\\anaconda3\\envs\\AI\\Lib\\site-packages\\ultralytics\\engine\\trainer.py:271: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(enabled=self.amp)\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\PlusninMA\\Desktop\\YOLOv8\\Accum.v1i.yolov8\\train\\labels.cache... 53 images, 0 backgrounds, 0 corrupt: 100%|██████████| 53/53 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\PlusninMA\\Desktop\\YOLOv8\\Accum.v1i.yolov8\\valid\\labels.cache... 17 images, 0 backgrounds, 0 corrupt: 100%|██████████| 17/17 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to C:\\Users\\PlusninMA\\Desktop\\YOLOv8\\yolov8n-seg_custom\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001667, momentum=0.9) with parameter groups 66 weight(decay=0.0), 77 weight(decay=0.0005), 76 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mC:\\Users\\PlusninMA\\Desktop\\YOLOv8\\yolov8n-seg_custom\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/10     0.554G      2.572       5.24      3.503      2.061          6        640: 100%|██████████| 27/27 [00:02<00:00,  9.48it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:00<00:00, 17.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         17        161     0.0478      0.727      0.107     0.0774     0.0474       0.72      0.105     0.0701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/10     0.552G      1.126      2.463      2.426      1.147          5        640: 100%|██████████| 27/27 [00:01<00:00, 22.98it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:00<00:00, 21.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         17        161      0.887      0.438      0.638      0.425      0.862      0.429       0.63      0.416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/10     0.556G      1.108       1.67      1.471      1.104          5        640: 100%|██████████| 27/27 [00:01<00:00, 24.80it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:00<00:00, 25.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         17        161      0.733      0.936      0.891      0.529      0.737      0.876       0.87      0.428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/10     0.552G      1.491      1.925      1.797       1.42          3        640: 100%|██████████| 27/27 [00:01<00:00, 24.84it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:00<00:00, 25.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         17        161      0.733      0.936      0.891      0.529      0.737      0.876       0.87      0.428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/10     0.556G      1.139      1.543      1.393      1.136         12        640: 100%|██████████| 27/27 [00:01<00:00, 23.10it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:00<00:00, 25.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         17        161      0.716      0.781      0.794      0.502      0.716      0.781      0.794      0.568\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/10     0.564G      0.969      1.383      1.229      1.055          5        640: 100%|██████████| 27/27 [00:01<00:00, 24.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:00<00:00, 23.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         17        161      0.757      0.851      0.867      0.601      0.757      0.851      0.865      0.602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/10     0.552G     0.9181      1.327      1.211      1.041         12        640: 100%|██████████| 27/27 [00:01<00:00, 24.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:00<00:00, 24.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         17        161      0.932      0.933       0.98      0.759      0.926      0.927      0.969      0.656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/10      0.56G      0.785      1.428      1.071     0.9746          4        640: 100%|██████████| 27/27 [00:01<00:00, 23.81it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:00<00:00, 25.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         17        161      0.902      0.972       0.97      0.794      0.902      0.972       0.97      0.709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/10     0.556G     0.7606      1.186      1.009      0.949          5        640: 100%|██████████| 27/27 [00:01<00:00, 23.84it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:00<00:00, 25.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         17        161      0.957       0.96      0.989      0.828      0.957       0.96      0.988      0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/10     0.552G     0.7128      1.327     0.9119     0.9307         12        640: 100%|██████████| 27/27 [00:01<00:00, 24.07it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:00<00:00, 25.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         17        161      0.957       0.96      0.989      0.828      0.957       0.96      0.988      0.726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 epochs completed in 0.015 hours.\n",
      "Optimizer stripped from C:\\Users\\PlusninMA\\Desktop\\YOLOv8\\yolov8n-seg_custom\\weights\\last.pt, 6.8MB\n",
      "Optimizer stripped from C:\\Users\\PlusninMA\\Desktop\\YOLOv8\\yolov8n-seg_custom\\weights\\best.pt, 6.8MB\n",
      "\n",
      "Validating C:\\Users\\PlusninMA\\Desktop\\YOLOv8\\yolov8n-seg_custom\\weights\\best.pt...\n",
      "Ultralytics YOLOv8.2.77  Python-3.12.4 torch-2.4.0+cu118 CUDA:0 (NVIDIA GeForce RTX 3060, 12288MiB)\n",
      "YOLOv8n-seg summary (fused): 195 layers, 3,258,454 parameters, 0 gradients, 12.0 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100%|██████████| 5/5 [00:00<00:00, 14.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         17        161      0.949      0.963      0.989      0.827      0.949      0.963      0.988      0.724\n",
      "                object         17        161      0.949      0.963      0.989      0.827      0.949      0.963      0.988      0.724\n",
      "Speed: 0.5ms preprocess, 4.8ms inference, 0.0ms loss, 3.5ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\Users\\PlusninMA\\Desktop\\YOLOv8\\yolov8n-seg_custom\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model_seg = YOLO('yolov8n-seg.pt')\n",
    " \n",
    "# Training.\n",
    "results = model_seg.train(\n",
    "   data='C:/Users/PlusninMA/Desktop/YOLOv8/Accum.v1i.yolov8/data.yaml',\n",
    "   imgsz=640,\n",
    "   epochs=10,\n",
    "   batch=2,\n",
    "   project=\"C:/Users/PlusninMA/Desktop/YOLOv8\",\n",
    "   name='yolov8n-seg_custom',    \n",
    "   save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e40463-e67f-4901-b594-1a0394ec5f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "# Загрузка модели YOLOv8\n",
    "model = YOLO('C:/Users/PlusninMA/Desktop/YOLOv8/yolov8n_custom/weights/best.pt')\n",
    "\n",
    "# Список цветов для различных классов\n",
    "colors = [\n",
    "    (255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0), (0, 255, 255),\n",
    "    (255, 0, 255), (192, 192, 192), (128, 128, 128), (128, 0, 0), (128, 128, 0),\n",
    "    (0, 128, 0), (128, 0, 128), (0, 128, 128), (0, 0, 128), (72, 61, 139),\n",
    "    (47, 79, 79), (47, 79, 47), (0, 206, 209), (148, 0, 211), (255, 20, 147)\n",
    "]\n",
    "\n",
    "# Открытие исходного видеофайла\n",
    "input_video_path = \"rtsp://admin:admin@192.168.120.127/1\"\n",
    "capture = cv2.VideoCapture(input_video_path)\n",
    "\n",
    "# Чтение параметров видео\n",
    "fps = int(capture.get(cv2.CAP_PROP_FPS))\n",
    "width = int(capture.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(capture.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Настройка выходного файла\n",
    "# output_video_path = 'detect.mp4'\n",
    "# fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "# writer = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "while True:\n",
    "    # Захват кадра\n",
    "    ret, frame = capture.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Обработка кадра с помощью модели YOLO\n",
    "    results = model(frame)[0]\n",
    "\n",
    "    # Получение данных об объектах\n",
    "    classes_names = results.names\n",
    "    classes = results.boxes.cls.cpu().numpy()\n",
    "    boxes = results.boxes.xyxy.cpu().numpy().astype(np.int32)\n",
    "    # Рисование рамок и подписей на кадре\n",
    "    for class_id, box, conf in zip(classes, boxes, results.boxes.conf):\n",
    "        if conf>0.5:\n",
    "            class_name = classes_names[int(class_id)]\n",
    "            color = colors[int(class_id) % len(colors)]\n",
    "            x1, y1, x2, y2 = box\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(frame, class_name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    # Запись обработанного кадра в выходной файл\n",
    "    # writer.write(frame)\n",
    "    cv2.imshow('Video', frame)\n",
    "        \n",
    "    if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Освобождение ресурсов и закрытие окон\n",
    "capture.release()\n",
    "writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f6c0a75-abbc-471d-b8db-59cac059efdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\PlusninMA\\Desktop\\camera_test\\section\\section_test\\normal_2.jpg: 640x480 12 objects, 38.0ms\n",
      "Speed: 2.0ms preprocess, 38.0ms inference, 5.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Results saved to \u001b[1mC:\\Users\\PlusninMA\\Desktop\\YOLOv8\\photo2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# model = YOLO('yolov8n.pt', 'v8')\n",
    "result = model.predict(\n",
    "   source='C:/Users/PlusninMA/Desktop/camera_test/section/section_test/normal_2.jpg',\n",
    "   conf=0.25,\n",
    "   project=\"C:/Users/PlusninMA/Desktop/YOLOv8\",\n",
    "   name=\"photo\",\n",
    "   save=True\n",
    ")\n",
    "# result.save(save_dir = \"C:/Users/PlusninMA/Desktop/YOLOv8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c67d6d-47da-4f1f-839a-44952d543db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(image_path)\n",
    "image_orig = image.copy()\n",
    "h_or, w_or = image.shape[:2]\n",
    "image = cv2.resize(image, (640, 640))\n",
    "results = model(image)[0]\n",
    "\n",
    "classes_names = results.names\n",
    "classes = results.boxes.cls.cpu().numpy()\n",
    "masks = results.masks.data.cpu().numpy()\n",
    "\n",
    "# Наложение масок на изображение\n",
    "for i, mask in enumerate(masks):\n",
    "    color = colors[int(classes[i]) % len(colors)]\n",
    "    \n",
    "    # Изменение размера маски перед созданием цветной маски\n",
    "    mask_resized = cv2.resize(mask, (w_or, h_or))\n",
    "    \n",
    "    # Создание цветной маски\n",
    "    color_mask = np.zeros((h_or, w_or, 3), dtype=np.uint8)\n",
    "    color_mask[mask_resized > 0] = color\n",
    "\n",
    "    # Сохранение маски каждого класса в отдельный файл\n",
    "    # mask_filename = os.path.join('results', f\"{classes_names[classes[i]]}_{i}.png\")\n",
    "    # cv2.imwrite(mask_filename, color_mask)\n",
    "\n",
    "    # Наложение маски на исходное изображение\n",
    "    image_orig = cv2.addWeighted(image_orig, 1.0, color_mask, 0.5, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
